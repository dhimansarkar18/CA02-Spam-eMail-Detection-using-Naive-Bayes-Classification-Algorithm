{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA_02_Dhiman_Sarkar",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "# 1. Importing the necessary modules and packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr"
      },
      "source": [
        "#Importing modules-packages\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPoIZEZYdUwY"
      },
      "source": [
        "# 2. Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pmiHWgdddMJ",
        "outputId": "b69d8cf9-f2a8-488a-f01b-533d1016fa33"
      },
      "source": [
        "#mounting google drive to link the working directories\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Brth0Up95L"
      },
      "source": [
        "# 3. Liniking Working Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIGGjocqp7YJ"
      },
      "source": [
        "test = \"/content/drive/MyDrive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails\"\n",
        "train = \"/content/drive/MyDrive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AExyJSsSjJ-T"
      },
      "source": [
        "# 4. Defining a function with 3000 most common words from all emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "def create_dictionary(root_dir):\n",
        "  all_words_list = []\n",
        "  #Listing files from email folder\n",
        "  emails_list = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  for email in emails_list:\n",
        "    with open(email) as eml:\n",
        "      for line in eml:\n",
        "#Splitting of lines into individual words\n",
        "        indv_word = line.split()\n",
        "#Appending the individual words to a list\n",
        "        all_words_list += indv_word\n",
        "#Count the number of words within the list\n",
        "  dictn = Counter(all_words_list)\n",
        "#Converting from count back to list\n",
        "  list_to_eliminate = list(dictn)\n",
        "\n",
        "  for l in list_to_eliminate:\n",
        "    if l.isalpha() == False:\n",
        "#Deletion of non-alphanumeric characters     \n",
        "      del dictn[l]\n",
        "    elif len(l) == 1:\n",
        "#Deletion of single-letterd word like \"I\"\n",
        "      del dictn[l]\n",
        "#Creating the final dictonary with 3000 most common words\n",
        "  dictn = dictn.most_common(3000)\n",
        "  return dictn          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL14EV31qwS5"
      },
      "source": [
        "# 5.Defining a function which extracts feature columns and populates their values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "def features_extracted (email_dir):\n",
        "#List of all files from folder\n",
        "  email = [os.path.join(email_dir,doc) for doc in os.listdir(email_dir)]\n",
        "#Making a list to store 3000 most common words and the number of files\n",
        "  features_matrix = np.zeros((len(email),3000))\n",
        "#Making an empty set of lables for the length of the number of files respectively\n",
        "  labels_train = np.zeros(len(email))\n",
        "  counts = 1;\n",
        "  fileID = 0;\n",
        "#For loop via listed files\n",
        "  for e in email:\n",
        "    with open(e) as il:\n",
        "#Enumerating of the looped file\n",
        "      for i, line in enumerate(il):\n",
        "        if i ==2:\n",
        "#Splitting of lines into individual words\n",
        "          word = line.split()\n",
        "          for w in word:\n",
        "#Base wordID            \n",
        "            wordID = 0\n",
        "            for il, d in enumerate(dictn):\n",
        "              if d[0] == w:\n",
        "#Matching the looped word with the word from the dictionary\n",
        "                wordID = il\n",
        "#Counting the number of occurances in case of a match\n",
        "                features_matrix[fileID,wordID] = word.count(word)\n",
        "#Base EmailID (not being spam)\n",
        "      labels_train[fileID] = 0;\n",
        "      filepathTokens = e.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "#In case name of the file indicates training data to be a spam, the email gets reallocated\n",
        "        labels_train[fileID] = 1;\n",
        "#In case the file is marked as a spam, the variable wordcount is changed. This is to insinuate the occurance of the word in spam file.\n",
        "        counts = counts + 1\n",
        "#Creating new fileID(EmailID) for the following email\n",
        "      fileID = fileID + 1\n",
        "  return features_matrix, labels_train             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuWbtkJuArTf"
      },
      "source": [
        "#6. Working Directories (Same as 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "source": [
        "test = \"/content/drive/MyDrive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails\"\n",
        "train = \"/content/drive/MyDrive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPCO_PF8Ib5l"
      },
      "source": [
        "# 7. Accuracy scores for predicting labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv_qBFzxJQn_"
      },
      "source": [
        "The accuracy score for spam emails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "76a86954-b8ff-45cd-edee-a6dbc349432a"
      },
      "source": [
        "#Base dictonary for most common words utilizing the training set\n",
        "dictn = create_dictionary(train)\n",
        "\n",
        "#Extraction of the ID and the respective counts from the two sets (test, training)\n",
        "print (\"Reading and Processing Emails from Train and Test folders\")\n",
        "features_matrix, labels_train = features_extracted(train)\n",
        "test_features_matrix, labels_test = features_extracted(test)\n",
        "\n",
        "#Gauss Naive Bayes Model (This model accounts the words to be normally distributed and are independent of each other)\n",
        "gnb_model = GaussianNB()\n",
        "\n",
        "#Check the features\n",
        "print (\"Training Model using Gaussian Naive Bayes Algorithm .....\")\n",
        "gnb_model.fit(features_matrix, labels_train)\n",
        "print (\"Training Complete\")\n",
        "print (\"Testing Trained model in order to predict Test Data labels\")\n",
        "labels_predicted = gnb_model.predict(test_features_matrix)\n",
        "print (\"Completed Test Data classification.... printing now Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(labels_test, labels_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading and Processing Emails from Train and Test folders\n",
            "Training Model using Gaussian Naive Bayes Algorithm .....\n",
            "Training Complete\n",
            "Testing Trained model in order to predict Test Data labels\n",
            "Completed Test Data classification.... printing now Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:450: RuntimeWarning: divide by zero encountered in log\n",
            "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:452: RuntimeWarning: invalid value encountered in true_divide\n",
            "  (self.sigma_[i, :]), 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}